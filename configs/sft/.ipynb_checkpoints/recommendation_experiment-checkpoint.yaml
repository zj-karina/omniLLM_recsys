# Recommendation experiment configuration
# Based on omni_qwen.ipynb experiment

# Basic parameters
model_name_or_path: "Qwen/Qwen2.5-Omni-7B"
model_type: "recommendation"  # New model type for recommendations

# Recommendation-specific parameters
id_vocab_size: 709036  # Size of item vocabulary (from Amazon Fashion 2023 dataset)
id_dim: 512             # Dimension of item embeddings
fusion_dim: 1024        # Hidden dimension for fusion head
reduced_dim: 1024       # Reduced dimension for text embeddings

# Dataset configuration
dataset_name: "seniichev/amazon-fashion-2023-full"
user_id_field: "user_id"
item_id_field: "parent_asin"
title_field: "title"
max_history_length: 10
min_history_length: 2

# Training parameters
output_dir: "./output/recommendation_model"
per_device_train_batch_size: 8
gradient_accumulation_steps: 1
num_train_epochs: 3
learning_rate: 1e-4
warmup_steps: 100
max_steps: 10000

# Optimization
gradient_checkpointing: true
bf16: true
dataloader_num_workers: 2
remove_unused_columns: false

# Logging
logging_steps: 10
save_steps: 1000  # Save model every 1000 steps
report_to: "wandb"
run_name: "recommendation_experiment"
log_file: "logs/recommendation_training.log"

# Early stopping
early_stopping_patience: 3
early_stopping_min_delta: 0.001
eval_steps: 1000  # Validate every 1000 steps

# Additional parameters
save_total_limit: 3
load_best_model_at_end: true
metric_for_best_model: "loss"

# Seed
seed: 42

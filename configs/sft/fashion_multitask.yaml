# Multi-task configuration for omnimodal LLM Amazon Fashion
# Training on multiple tasks: recommendations, analysis, comparison, personalization

# Basic parameters
model_name_or_path: "Qwen/Qwen2.5-7B-Instruct"
modalities:
  input: ["image", "text"]
  output: ["text"]

# Vision encoder
encoders:
  image:
    model: "openai/clip-vit-large-patch14"
    freeze: true
    tokenizer_type: "discrete"

# Projection layer
projection:
  type: "mlp"
  hidden_size: 1024
  num_layers: 2
  dropout: 0.1

# Dataset - using existing quick_start_dataset
dataset: ["fashion_multitask_dataset/conversations/train.jsonl"]
conversation_field: "conversations"
max_seq_length: 4096

# Training parameters
output_dir: "./output/fashion_multitask_model"
per_device_train_batch_size: 2
gradient_accumulation_steps: 8
num_train_epochs: 3
learning_rate: 2e-5
warmup_steps: 100
max_steps: 3000

# Components
freeze_vision_encoder: true
freeze_llm: false
train_projection_only: false

# Optimization
gradient_checkpointing: true
bf16: true
dataloader_num_workers: 2
remove_unused_columns: false

# Special tokens
special_tokens:
  image_start: "<img>"
  image_end: "</img>"

# Logging
logging_steps: 50
save_steps: 500
eval_steps: 500
evaluation_strategy: "steps"
report_to: "none"
run_name: "fashion_multitask_training"

# Additional parameters
save_total_limit: 3
load_best_model_at_end: true
metric_for_best_model: "loss"

# Generation
generate_eval_examples: true
max_new_tokens: 256

# Seed
seed: 42

# Task weights
task_weights:
  product_analysis: 1.0
  next_purchase_recommendation: 1.2
  product_comparison: 1.0
  personalized_recommendation: 1.1
  review_generation: 0.8




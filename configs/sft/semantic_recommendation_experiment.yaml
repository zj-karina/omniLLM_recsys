# Safe semantic recommendation experiment configuration
# Conservative settings to avoid NaN loss

# Basic parameters
model_name_or_path: "Qwen/Qwen2.5-Omni-7B"
model_type: "semantic_recommendation"  # Model type with VQ-VAE semantic IDs

# Recommendation-specific parameters
id_vocab_size: 709036  # Size of item vocabulary (from Amazon Fashion 2023 dataset)
id_dim: 512             # Dimension of item embeddings
fusion_dim: 1024        # Hidden dimension for fusion head
reduced_dim: 1024       # Reduced dimension for text embeddings

# VQ-VAE parameters for semantic IDs
vq_codebook_size: 10000  # Size of VQ-VAE codebook
vq_codebook_dim: 256     # Dimension of VQ-VAE codes
use_semantic_ids: true   # Whether to use VQ-VAE semantic IDs

# Dataset configuration
dataset_name: "seniichev/amazon-fashion-2023-full"
user_id_field: "user_id"
item_id_field: "parent_asin"
title_field: "title"
max_history_length: 10
min_history_length: 2

# Training parameters - CONSERVATIVE SETTINGS
output_dir: "./output/semantic_recommendation_model_safe"
per_device_train_batch_size: 1  # Smaller batch size
gradient_accumulation_steps: 16  # More accumulation
num_train_epochs: 1  # Fewer epochs for testing
learning_rate: 1e-5  # Much lower learning rate
warmup_steps: 200  # More warmup
max_steps: 1000  # Fewer steps for testing
max_grad_norm: 0.5  # Stronger gradient clipping

# Optimization
gradient_checkpointing: true
bf16: false  # Use fp32 for stability
fp16: false  # Disable mixed precision
dataloader_num_workers: 1  # Fewer workers
remove_unused_columns: false

# Logging
logging_steps: 5  # More frequent logging
save_steps: 100  # Save more frequently
report_to: "wandb"
run_name: "semantic_recommendation_safe"
log_file: "logs/semantic_recommendation_safe.log"

# Early stopping - DISABLED for testing
early_stopping_patience: 0
early_stopping_min_delta: 0.001
eval_steps: 0  # Disable validation
evaluation_strategy: "no"  # Disable evaluation

# Additional parameters
save_total_limit: 2
load_best_model_at_end: false
metric_for_best_model: "loss"

# Seed
seed: 42


#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ Hugging Face Hub.
"""

import os
import shutil
import json
from pathlib import Path
from huggingface_hub import HfApi, Repository
import torch

def create_model_config(model_path: str, model_type: str) -> dict:
    """–°–æ–∑–¥–∞–µ—Ç config.json –¥–ª—è –º–æ–¥–µ–ª–∏."""
    
    if model_type == "fashion_multitask":
        config = {
            "model_type": "multimodal",
            "base_model": "Qwen/Qwen2.5-7B-Instruct",
            "vision_encoder": "openai/clip-vit-large-patch14",
            "modalities": {
                "input": ["image", "text"],
                "output": ["text"]
            },
            "projection": {
                "type": "mlp",
                "hidden_size": 1024,
                "num_layers": 2,
                "dropout": 0.1
            },
            "max_seq_length": 4096,
            "special_tokens": {
                "image_start": "<img>",
                "image_end": "</img>"
            }
        }
    elif model_type == "recommendation":
        config = {
            "model_type": "recommendation",
            "base_model": "Qwen/Qwen2.5-Omni-7B",
            "id_vocab_size": 709036,
            "id_dim": 512,
            "fusion_dim": 1024,
            "reduced_dim": 1024,
            "dataset": "seniichev/amazon-fashion-2023-full",
            "max_history_length": 10,
            "min_history_length": 2
        }
    elif model_type == "semantic_recommendation":
        config = {
            "model_type": "semantic_recommendation",
            "base_model": "Qwen/Qwen2.5-Omni-7B",
            "id_vocab_size": 709036,
            "id_dim": 512,
            "fusion_dim": 1024,
            "reduced_dim": 1024,
            "vq_codebook_size": 10000,
            "vq_codebook_dim": 256,
            "use_semantic_ids": True,
            "dataset": "seniichev/amazon-fashion-2023-full",
            "max_history_length": 10,
            "min_history_length": 2
        }
    
    return config

def create_readme(model_type: str, repo_name: str) -> str:
    """–°–æ–∑–¥–∞–µ—Ç README.md –¥–ª—è —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è."""
    
    if model_type == "fashion_multitask":
        return f"""# {repo_name}

Multimodal Large Language Model –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π Amazon Fashion.

## –û–ø–∏—Å–∞–Ω–∏–µ

–≠—Ç–∞ –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ –º—É–ª—å—Ç–∏–∑–∞–¥–∞—á–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ Amazon Fashion –∏ –º–æ–∂–µ—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å —Å–ª–µ–¥—É—é—â–∏–µ –∑–∞–¥–∞—á–∏:
- –ê–Ω–∞–ª–∏–∑ —Ç–æ–≤–∞—Ä–æ–≤
- –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å–ª–µ–¥—É—é—â–∏—Ö –ø–æ–∫—É–ø–æ–∫
- –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–æ–≤–∞—Ä–æ–≤
- –ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–∑—ã–≤–æ–≤

## –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

- **–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å**: Qwen2.5-7B-Instruct
- **–í–∏–∑—É–∞–ª—å–Ω—ã–π —ç–Ω–∫–æ–¥–µ—Ä**: CLIP-ViT-Large-Patch14
- **–ü—Ä–æ–µ–∫—Ü–∏–æ–Ω–Ω—ã–π —Å–ª–æ–π**: MLP (1024 —Å–∫—Ä—ã—Ç—ã—Ö –µ–¥–∏–Ω–∏—Ü, 2 —Å–ª–æ—è)
- **–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏**: 4096 —Ç–æ–∫–µ–Ω–æ–≤

## –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

```python
from transformers import AutoTokenizer, AutoModel
import torch

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
model = AutoModel.from_pretrained("{repo_name}")
tokenizer = AutoTokenizer.from_pretrained("{repo_name}")

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
text = "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–æ—Ç —Ç–æ–≤–∞—Ä –º–æ–¥–Ω–æ–π –æ–¥–µ–∂–¥—ã"
# –î–æ–±–∞–≤—å—Ç–µ –∫–æ–¥ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ç–µ–∫—Å—Ç–∞
```

## –û–±—É—á–µ–Ω–∏–µ

–ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ Amazon Fashion —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Any2Any Trainer.

## –õ–∏—Ü–µ–Ω–∑–∏—è

Apache-2.0 License
"""
    
    elif model_type == "recommendation":
        return f"""# {repo_name}

–ú–æ–¥–µ–ª—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ ID –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è Amazon Fashion.

## –û–ø–∏—Å–∞–Ω–∏–µ

–≠—Ç–∞ –º–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç ID —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ç–æ–≤–∞—Ä–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏–∏ –ø–æ–∫—É–ø–æ–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π.

## –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

- **–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å**: Qwen2.5-Omni-7B
- **–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è —Ç–æ–≤–∞—Ä–æ–≤**: 709,036
- **–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å ID —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤**: 512
- **–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å fusion head**: 1024
- **–î–∞—Ç–∞—Å–µ—Ç**: Amazon Fashion 2023 Full

## –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

```python
from any2any_trainer.models.recommendation import RecommendationModel

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
model = RecommendationModel.from_pretrained("{repo_name}")

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
recommendations = model.predict_next_item(
    text="–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∫—É–ø–∏–ª –¥–∂–∏–Ω—Å—ã –∏ —Ñ—É—Ç–±–æ–ª–∫—É",
    id_ids=[12345, 67890],  # ID —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏
    top_k=5
)
```

## –û–±—É—á–µ–Ω–∏–µ

–ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ Amazon Fashion 2023 —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º ID –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏.

## –õ–∏—Ü–µ–Ω–∑–∏—è

Apache-2.0 License
"""
    
    elif model_type == "semantic_recommendation":
        return f"""# {repo_name}

–ú–æ–¥–µ–ª—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º–∏ ID –¥–ª—è Amazon Fashion.

## –û–ø–∏—Å–∞–Ω–∏–µ

–≠—Ç–∞ –º–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç VQ-VAE –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö ID —Ç–æ–≤–∞—Ä–æ–≤, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ –ø–æ–Ω–∏–º–∞—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏ –º–µ–∂–¥—É —Ç–æ–≤–∞—Ä–∞–º–∏.

## –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

- **–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å**: Qwen2.5-Omni-7B
- **–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è —Ç–æ–≤–∞—Ä–æ–≤**: 709,036
- **–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å ID —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤**: 512
- **VQ-VAE codebook size**: 10,000
- **VQ-VAE codebook dimension**: 256
- **–î–∞—Ç–∞—Å–µ—Ç**: Amazon Fashion 2023 Full

## –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

```python
from any2any_trainer.models.recommendation import SemanticIDRecommendationModel

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
model = SemanticIDRecommendationModel.from_pretrained("{repo_name}")

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º–∏ ID
recommendations = model.predict_next_item(
    text="–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∫—É–ø–∏–ª –¥–∂–∏–Ω—Å—ã –∏ —Ñ—É—Ç–±–æ–ª–∫—É",
    id_ids=[12345, 67890],  # ID —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏
    top_k=5,
    use_semantic_ids=True
)
```

## –û–±—É—á–µ–Ω–∏–µ

–ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ Amazon Fashion 2023 —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö ID —á–µ—Ä–µ–∑ VQ-VAE.

## –õ–∏—Ü–µ–Ω–∑–∏—è

Apache-2.0 License
"""

def prepare_model_for_upload(model_path: str, model_type: str, repo_name: str) -> str:
    """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏."""
    
    # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–∞–ø–∫—É –¥–ª—è –º–æ–¥–µ–ª–∏
    temp_dir = f"temp_{model_type}_model"
    os.makedirs(temp_dir, exist_ok=True)
    
    # –ö–æ–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –º–æ–¥–µ–ª–∏
    if os.path.exists(f"{model_path}/checkpoint-10000"):
        checkpoint_path = f"{model_path}/checkpoint-10000"
    elif os.path.exists(f"{model_path}/checkpoint-30000"):
        checkpoint_path = f"{model_path}/checkpoint-30000"
    elif os.path.exists(f"{model_path}/checkpoint-4200"):
        checkpoint_path = f"{model_path}/checkpoint-4200"
    else:
        # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç
        checkpoints = [d for d in os.listdir(model_path) if d.startswith("checkpoint-")]
        if checkpoints:
            checkpoint_path = f"{model_path}/{max(checkpoints, key=lambda x: int(x.split('-')[1]))}"
        else:
            raise ValueError(f"–ù–µ –Ω–∞–π–¥–µ–Ω —á–µ–∫–ø–æ–∏–Ω—Ç –≤ {model_path}")
    
    print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç: {checkpoint_path}")
    
    # –ö–æ–ø–∏—Ä—É–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
    for file in os.listdir(checkpoint_path):
        src = os.path.join(checkpoint_path, file)
        dst = os.path.join(temp_dir, file)
        if os.path.isfile(src):
            shutil.copy2(src, dst)
    
    # –°–æ–∑–¥–∞–µ–º config.json
    config = create_model_config(model_path, model_type)
    with open(f"{temp_dir}/config.json", "w", encoding="utf-8") as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    
    # –°–æ–∑–¥–∞–µ–º README.md
    readme_content = create_readme(model_type, repo_name)
    with open(f"{temp_dir}/README.md", "w", encoding="utf-8") as f:
        f.write(readme_content)
    
    return temp_dir

def upload_model(model_path: str, repo_name: str, model_type: str):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ Hugging Face Hub."""
    
    print(f"\nüöÄ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ {model_type} –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏...")
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å
    temp_dir = prepare_model_for_upload(model_path, model_type, repo_name)
    
    print(f"üìÅ –í—Ä–µ–º–µ–Ω–Ω–∞—è –ø–∞–ø–∫–∞ —Å–æ–∑–¥–∞–Ω–∞: {temp_dir}")
    print(f"üìã –§–∞–π–ª—ã –≤ –ø–∞–ø–∫–µ: {os.listdir(temp_dir)}")
    
    # –°–æ–∑–¥–∞–µ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π
    print(f"\nüì§ –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è {repo_name}...")
    
    try:
        api = HfApi()
        
        # –°–æ–∑–¥–∞–µ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        try:
            api.create_repo(repo_id=repo_name, exist_ok=True)
            print(f"‚úÖ –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π {repo_name} —Å–æ–∑–¥–∞–Ω/–æ–±–Ω–æ–≤–ª–µ–Ω")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è: {e}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª—ã
        print(f"üì§ –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤ –≤ {repo_name}...")
        
        for file in os.listdir(temp_dir):
            file_path = os.path.join(temp_dir, file)
            if os.path.isfile(file_path):
                print(f"  –ó–∞–≥—Ä—É–∂–∞–µ–º {file}...")
                api.upload_file(
                    path_or_fileobj=file_path,
                    path_in_repo=file,
                    repo_id=repo_name,
                    repo_type="model"
                )
        
        print(f"‚úÖ –ú–æ–¥–µ–ª—å {model_type} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –≤ {repo_name}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {model_type}: {e}")
        return False
    
    finally:
        # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–∞–ø–∫—É
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)
            print(f"üßπ –í—Ä–µ–º–µ–Ω–Ω–∞—è –ø–∞–ø–∫–∞ {temp_dir} —É–¥–∞–ª–µ–Ω–∞")
    
    return True

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    
    print("üéØ –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ï–ô –ù–ê HUGGING FACE HUB")
    print("=" * 50)
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
    models_config = [
        {
            "model_path": "output/fashion_multitask_model",
            "repo_name": "zjkarina/omniRecsysLLM_fasion",
            "model_type": "fashion_multitask"
        },
        {
            "model_path": "output/recommendation_model", 
            "repo_name": "zjkarina/omniRecsysLLM_idmodality",
            "model_type": "recommendation"
        },
        {
            "model_path": "output/semantic_recommendation_model_safe",
            "repo_name": "zjkarina/omniRecsysLLM_semanticIDsmodality", 
            "model_type": "semantic_recommendation"
        }
    ]
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ –º–æ–¥–µ–ª–∏ —Å—É—â–µ—Å—Ç–≤—É—é—Ç
    for config in models_config:
        if not os.path.exists(config["model_path"]):
            print(f"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {config['model_path']}")
            return
    
    print("‚úÖ –í—Å–µ –º–æ–¥–µ–ª–∏ –Ω–∞–π–¥–µ–Ω—ã")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–∞–∂–¥—É—é –º–æ–¥–µ–ª—å
    success_count = 0
    for config in models_config:
        print(f"\n{'='*20} {config['model_type'].upper()} {'='*20}")
        
        if upload_model(
            config["model_path"],
            config["repo_name"], 
            config["model_type"]
        ):
            success_count += 1
            print(f"‚úÖ {config['model_type']} –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {config['model_type']}")
    
    print(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢: {success_count}/{len(models_config)} –º–æ–¥–µ–ª–µ–π –∑–∞–≥—Ä—É–∂–µ–Ω–æ")
    
    if success_count == len(models_config):
        print("\nüéâ –í—Å–µ –º–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –Ω–∞ Hugging Face Hub!")
        print("\n–°—Å—ã–ª–∫–∏ –Ω–∞ –º–æ–¥–µ–ª–∏:")
        for config in models_config:
            print(f"  - https://huggingface.co/{config['repo_name']}")
    else:
        print(f"\n‚ö†Ô∏è –ó–∞–≥—Ä—É–∂–µ–Ω–æ {success_count} –∏–∑ {len(models_config)} –º–æ–¥–µ–ª–µ–π")

if __name__ == "__main__":
    main()
